{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hate_speech_offensive (/root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3491af5c05274cf5b4153099c3a9d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('hate_speech_offensive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
       "0      3                  0                         0              3      2   \n",
       "1      3                  0                         3              0      1   \n",
       "2      3                  0                         3              0      1   \n",
       "3      3                  0                         2              1      1   \n",
       "4      6                  0                         6              0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopword=set(stopwords.words('english'))\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>rt mayasolov woman shouldnt complain clean ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rt  boy dat coldtyga dwn bad cuffin dat hoe  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rt urkindofbrand dawg rt  ever fuck bitch sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>rt cganderson vivabas look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rt shenikarobert shit hear might true might f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
       "0      3                  0                         0              3      2   \n",
       "1      3                  0                         3              0      1   \n",
       "2      3                  0                         3              0      1   \n",
       "3      3                  0                         2              1      1   \n",
       "4      6                  0                         6              0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0   rt mayasolov woman shouldnt complain clean ho...  \n",
       "1   rt  boy dat coldtyga dwn bad cuffin dat hoe  ...  \n",
       "2   rt urkindofbrand dawg rt  ever fuck bitch sta...  \n",
       "3             rt cganderson vivabas look like tranni  \n",
       "4   rt shenikarobert shit hear might true might f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet']=df['tweet'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826,) (4957,) (19826,) (4957,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 51\n",
    "test_size = 0.2\n",
    "X = df['tweet']\n",
    "y = df['class']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.2,random_state=seed,stratify=df['class'])\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF ML Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasforming the data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19826, 5000), (4957, 5000))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape, X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(clf, X_train, y_train, X_test):\n",
    "    \"\"\" with clf, make pred, return pred\"\"\"\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.16      0.25       286\n",
      "           1       0.91      0.97      0.94      3838\n",
      "           2       0.87      0.83      0.85       833\n",
      "\n",
      "    accuracy                           0.90      4957\n",
      "   macro avg       0.77      0.65      0.68      4957\n",
      "weighted avg       0.88      0.90      0.88      4957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#instantiate the models with default hyper-parameters\n",
    "logreg = LogisticRegression()\n",
    "logreg_predictions = make_pred(logreg, X_train_transformed, y_train, X_test_transformed)\n",
    "print(classification_report(y_test, logreg_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  46,  216,   24],\n",
       "       [  38, 3717,   83],\n",
       "       [   4,  140,  689]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, logreg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.01       286\n",
      "           1       0.83      1.00      0.90      3838\n",
      "           2       0.92      0.37      0.52       833\n",
      "\n",
      "    accuracy                           0.83      4957\n",
      "   macro avg       0.75      0.46      0.48      4957\n",
      "weighted avg       0.82      0.83      0.79      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_predictions = make_pred(nb_model, X_train_transformed, y_train, X_test_transformed)\n",
    "print(classification_report(y_test, nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  46,  216,   24],\n",
       "       [  38, 3717,   83],\n",
       "       [   4,  140,  689]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, logreg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model= xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc' )\n",
    "\n",
    "xgb_predictions = make_pred(xgb_model, X_train_transformed, y_train, X_test_transformed)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  50,  199,   37],\n",
       "       [  33, 3609,  196],\n",
       "       [   2,   38,  793]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'n_estimators' : [40, 60, 80, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "xgb_model= xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=3, min_child_weight=5, n_estimators=40, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=3, min_child_weight=5, n_estimators=40, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=3, min_child_weight=5, n_estimators=40, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, gamma=1, max_depth=3, min_child_weight=10, n_estimators=40, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=1, n_estimators=60, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, gamma=1, max_depth=3, min_child_weight=10, n_estimators=40, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=1, n_estimators=60, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=1, n_estimators=60, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, gamma=1, max_depth=3, min_child_weight=10, n_estimators=40, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=1.0, gamma=5, max_depth=4, min_child_weight=1, n_estimators=40, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.5, max_depth=4, min_child_weight=10, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.5, max_depth=4, min_child_weight=10, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, gamma=5, max_depth=4, min_child_weight=1, n_estimators=40, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.5, max_depth=4, min_child_weight=10, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=1.0, gamma=5, max_depth=4, min_child_weight=1, n_estimators=40, subsample=1.0; total time=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=&lt;generator object _BaseKFold.split at 0x7fd1a457fa50&gt;,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=&#x27;auc&#x27;,\n",
       "                                           feature_types=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_typ...\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=80, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=4,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.6, 0.8, 1.0],\n",
       "                                        &#x27;gamma&#x27;: [0.5, 1, 1.5, 2, 5],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                                        &#x27;min_child_weight&#x27;: [1, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [40, 60, 80, 100],\n",
       "                                        &#x27;subsample&#x27;: [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring=&#x27;f1_micro&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=&lt;generator object _BaseKFold.split at 0x7fd1a457fa50&gt;,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=&#x27;auc&#x27;,\n",
       "                                           feature_types=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_typ...\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=80, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=4,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.6, 0.8, 1.0],\n",
       "                                        &#x27;gamma&#x27;: [0.5, 1, 1.5, 2, 5],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                                        &#x27;min_child_weight&#x27;: [1, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [40, 60, 80, 100],\n",
       "                                        &#x27;subsample&#x27;: [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring=&#x27;f1_micro&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=80, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=80, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x7fd1a457fa50>,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric='auc',\n",
       "                                           feature_types=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_typ...\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=80, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=4,\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                                        'max_depth': [3, 4, 5],\n",
       "                                        'min_child_weight': [1, 5, 10],\n",
       "                                        'n_estimators': [40, 60, 80, 100],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring='f1_micro', verbose=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=params, \n",
    "                                   n_iter=param_comb, \n",
    "                                   scoring='f1_micro', n_jobs=4, \n",
    "                                   cv=skf.split(X_train_transformed, y_train), \n",
    "                                   verbose=2, random_state=1001 )\n",
    "\n",
    "random_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.48397613, 1.34970284, 0.48469257, 1.69550864, 0.76583099]), 'std_fit_time': array([0.0308516 , 0.1319969 , 0.0665246 , 0.28342226, 0.15450312]), 'mean_score_time': array([0.02062607, 0.02685388, 0.01780701, 0.02830609, 0.01704741]), 'std_score_time': array([0.00669257, 0.00768931, 0.0075051 , 0.01292306, 0.00919365]), 'param_subsample': masked_array(data=[0.8, 1.0, 0.8, 0.8, 1.0],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[40, 60, 40, 100, 40],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[5, 1, 10, 10, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[3, 4, 3, 4, 4],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[2, 2, 1, 1.5, 5],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_colsample_bytree': masked_array(data=[0.8, 0.8, 1.0, 0.6, 1.0],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'subsample': 0.8, 'n_estimators': 40, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'n_estimators': 60, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'n_estimators': 40, 'min_child_weight': 10, 'max_depth': 3, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 4, 'gamma': 1.5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'n_estimators': 40, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 5, 'colsample_bytree': 1.0}], 'split0_test_score': array([0.84127705, 0.88379483, 0.84006658, 0.88636708, 0.87395975]), 'split1_test_score': array([0.84188228, 0.88666969, 0.845665  , 0.88939325, 0.87880163]), 'split2_test_score': array([0.83610775, 0.87953995, 0.83535109, 0.88559322, 0.87273002]), 'mean_test_score': array([0.83975569, 0.88333482, 0.84036089, 0.88711785, 0.8751638 ]), 'std_test_score': array([0.00259129, 0.00292882, 0.00421578, 0.00163968, 0.00262087]), 'rank_test_score': array([5, 2, 4, 1, 3], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.19      0.28       286\n",
      "           1       0.94      0.94      0.94      3838\n",
      "           2       0.77      0.96      0.85       833\n",
      "\n",
      "    accuracy                           0.90      4957\n",
      "   macro avg       0.74      0.69      0.69      4957\n",
      "weighted avg       0.89      0.90      0.89      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_xgb_predictions = random_search.predict(X_test_transformed)\n",
    "print(classification_report(y_test, best_xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  54,  191,   41],\n",
       "       [  49, 3590,  199],\n",
       "       [   0,   37,  796]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, best_xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(clf, texts):\n",
    "    id_to_label = {0: 'hate-speech', 1: 'offensive-language', 2: 'neither'}\n",
    "    transformed_texts = vectorizer.transform(texts)\n",
    "    preds = clf.predict(transformed_texts)\n",
    "    for text, pred in zip(texts, preds):\n",
    "        print(f'Sent: {text}, Pred: {id_to_label[pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: i fucking hate you, piece of shit, Pred: offensive-language\n"
     ]
    }
   ],
   "source": [
    "inference(random_search, ['i fucking hate you, piece of shit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: youre so handsome, Pred: neither\n"
     ]
    }
   ],
   "source": [
    "inference(random_search, ['youre so handsome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on pit2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Id</th>\n",
       "      <th>Topic_Name</th>\n",
       "      <th>Sent_1</th>\n",
       "      <th>Sent_2</th>\n",
       "      <th>Label</th>\n",
       "      <th>Sent_1_tag</th>\n",
       "      <th>Sent_2_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>8 mile is on thats my movie</td>\n",
       "      <td>3</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>8/O/NN/B-NP/O mile/O/NN/I-NP/O is/O/VBZ/B-VP/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>The last rap battle in 8 Mile nevr gets old ahah</td>\n",
       "      <td>2</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>The rap battle at the end of 8 mile gets me so...</td>\n",
       "      <td>2</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Id Topic_Name                               Sent_1  \\\n",
       "0        51     8 Mile  All the home alones watching 8 mile   \n",
       "1        51     8 Mile  All the home alones watching 8 mile   \n",
       "2        51     8 Mile  All the home alones watching 8 mile   \n",
       "\n",
       "                                              Sent_2  Label  \\\n",
       "0                        8 mile is on thats my movie      3   \n",
       "1   The last rap battle in 8 Mile nevr gets old ahah      2   \n",
       "2  The rap battle at the end of 8 mile gets me so...      2   \n",
       "\n",
       "                                          Sent_1_tag  \\\n",
       "0  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "1  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "2  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "\n",
       "                                          Sent_2_tag  \n",
       "0  8/O/NN/B-NP/O mile/O/NN/I-NP/O is/O/VBZ/B-VP/O...  \n",
       "1  The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...  \n",
       "2  The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_path = '/workspace/grasp-data-hometask-semantic-similarity-master/data/test.data'\n",
    "col_names = ['Topic_Id', 'Topic_Name', 'Sent_1', 'Sent_2', 'Label', 'Sent_1_tag', 'Sent_2_tag']\n",
    "test_df = pd.read_csv(test_path, sep='\\t', lineterminator='\\n', names=col_names, header=None)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = test_df['Sent_2'].tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: 8 mile is on thats my movie, Pred: neither\n",
      "Sent: The last rap battle in 8 Mile nevr gets old ahah, Pred: neither\n",
      "Sent: The rap battle at the end of 8 mile gets me so hype, Pred: neither\n",
      "Sent: Rabbit on 8 mile out of place but determined to make it, Pred: neither\n",
      "Sent: See 8 Mile is always on but it s the tv version so it s gay, Pred: neither\n"
     ]
    }
   ],
   "source": [
    "inference(random_search, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: Goin to see after earth with the fam, Pred: neither\n",
      "Sent: will smith s speech in after earth is so relevant, Pred: neither\n",
      "Sent: Just got done eating chinese with the fam now ganna go see after earth, Pred: neither\n",
      "Sent: After earth is out and I havent seen it yet, Pred: neither\n",
      "Sent: wanted to watch After Earth today, Pred: neither\n",
      "Sent: Finally in the theaters to see after earth, Pred: neither\n",
      "Sent: the hangover 3 and after earth are both really good, Pred: neither\n",
      "Sent: I kinda wanna see After Earth as well, Pred: neither\n",
      "Sent: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6, Pred: neither\n",
      "Sent: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast, Pred: neither\n"
     ]
    }
   ],
   "source": [
    "inference(random_search, test_df['Sent_2'].tolist()[50:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: Those last 3 battles in 8 Mile are THE shit, Pred: offensive-language\n",
      "Sent: After Earth is a great ass movie, Pred: offensive-language\n",
      "Sent: Benitez is alright tho man fuck chelsea fans they suck asshole, Pred: offensive-language\n",
      "Sent: He can fuck up the Big 12 all he wants, Pred: offensive-language\n",
      "Sent: So Wiggins Is Settling For Playing In The Garbage Ass Big 12, Pred: offensive-language\n",
      "Sent: Spo aint in the game chalmers, Pred: offensive-language\n",
      "Sent: Lucky ass shxt by Chalmers, Pred: offensive-language\n",
      "Sent: The fuck Chalmers is doing, Pred: offensive-language\n",
      "Sent: Why is chara playing like a bitch, Pred: offensive-language\n",
      "Sent: Oh shit I gotta try that new ciroc flavor, Pred: offensive-language\n",
      "Sent: New Ciroc flavor on the market gotta try that shit, Pred: offensive-language\n",
      "Sent: Ciroc is shit vodka anyway, Pred: offensive-language\n",
      "Sent: but yall so damn hype bout the new ciroc, Pred: offensive-language\n",
      "Sent: I swear Fuck Family Guy for being that funny tonight, Pred: offensive-language\n",
      "Sent: You got this shit in the bag love the game 7 dramatic finish, Pred: offensive-language\n",
      "Sent: The White Sox just gave up a Grand Slam to a pitcher, Pred: offensive-language\n",
      "Sent: Some crazy shit must be happening in Japan at the moment, Pred: offensive-language\n",
      "Sent: NICKIMINAJ is Lydia a bad bitch or just a dorky assistant, Pred: offensive-language\n",
      "Sent: Aint nobody gonna play Lydia tho, Pred: offensive-language\n",
      "Sent: Lydia is a cute as fuck name tho, Pred: offensive-language\n",
      "Sent: i knew some little girl name Lydia her fast ass, Pred: offensive-language\n",
      "Sent: lydia is already famous shit, Pred: offensive-language\n",
      "Sent: make that white button blue for me please Niall, Pred: offensive-language\n",
      "Sent: Roberto Mancini just got sacked after that shit season, Pred: offensive-language\n",
      "Sent: sounds like a total kick ass series, Pred: offensive-language\n",
      "Sent: Fuckin Star Wars is on and you wanna talk on the phone, Pred: offensive-language\n",
      "Sent: ahhhh shit my dad got me Taco Bell, Pred: offensive-language\n",
      "Sent: So the purge aint worth it, Pred: offensive-language\n",
      "Sent: It s about to the purge up in this bitch, Pred: offensive-language\n",
      "Sent: Thornton is such a bitch for a big guy, Pred: offensive-language\n",
      "Sent: Thornton is just a shit chris neil, Pred: offensive-language\n",
      "Sent: Damn zbo got crossed up, Pred: offensive-language\n"
     ]
    }
   ],
   "source": [
    "def filter(clf, texts, id):\n",
    "    id_to_label = {0: 'hate-speech', 1: 'offensive-language', 2: 'neither'}\n",
    "    transformed_texts = vectorizer.transform(texts)\n",
    "    preds = clf.predict(transformed_texts)\n",
    "    for text, pred in zip(texts, preds):\n",
    "        if pred == id:\n",
    "            print(f'Sent: {text}, Pred: {id_to_label[pred]}')\n",
    "\n",
    "filter(random_search, test_df['Sent_2'].tolist(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 10:36:41.504314: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 10:36:41.734548: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-16 10:36:42.398578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-10-16 10:36:42.398667: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-10-16 10:36:42.398676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=51, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        lab = [0.] * 3\n",
    "        lab[self.labels[idx]] = 1.\n",
    "        item['labels'] = torch.tensor(lab)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HateDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = HateDataset(test_encodings, y_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,   279, 28609,  3967,   118,  8446, 18657,  3245,   146, 18940,\n",
       "         26138,  9013, 32594, 33976,  5384,  1780,  8987, 35468,   405,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([1., 0., 0.])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hatespeech_results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    metric_for_best_model=\"f1\",  # select the base metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, recall_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    predictions = sigmoid(torch.Tensor(predictions))\n",
    "    predictions = torch.argmax(predictions, dim=1)\n",
    "    labels = torch.argmax(torch.Tensor(labels), dim=1)\n",
    "    #print(predictions)\n",
    "    #print(labels)\n",
    "\n",
    "    f1_micro_average = f1_score(y_true=labels, y_pred=predictions, average='micro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'accuracy': accuracy,\n",
    "               'recall': recall}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as mlrun:\n",
    "    trainer.train()\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/workspace/grasp-data-hometask-semantic-similarity-master/scripts/hatespeech_results/checkpoint-3500\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hatespeech_results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    metric_for_best_model=\"f1\",  # select the base metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2431873381137848, 'eval_f1': 0.8539439176921525, 'eval_accuracy': 0.8539439176921525, 'eval_recall': 0.8539439176921525, 'eval_runtime': 5.2193, 'eval_samples_per_second': 949.748, 'eval_steps_per_second': 118.79}\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "with mlflow.start_run() as mlrun:\n",
    "    print(trainer.evaluate())\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
